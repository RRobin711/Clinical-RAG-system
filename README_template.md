# ðŸ¥ Clinical Psychology RAG System

A domain-specific Retrieval-Augmented Generation system for querying and summarizing clinical psychology research literature, with a focus on validated assessment scales (MADRS, PHQ-9, ADHD-RS).

**[Live Demo](https://huggingface.co/spaces/YOUR_USERNAME/clinical-rag)** | **[Blog Post](https://medium.com/@YOUR_USERNAME/what-broke-when-i-built-a-clinical-rag-system)** | **[W&B Dashboard](https://wandb.ai/YOUR_USERNAME/clinical-rag)**

---

## What This Does

Ask questions about clinical assessment scales and get grounded, cited answers from research literature:

```
Q: "What is the inter-rater reliability of the MADRS?"
A: "The MADRS demonstrates excellent inter-rater reliability, with ICC values 
   ranging from 0.89 to 0.97 across validation studies [chunk_abc_00012]. 
   Smith et al. reported ICC = 0.89 (95% CI: 0.82â€“0.94) in a primary care 
   sample of 200 participants [chunk_def_00003]."
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    QUERY PIPELINE                             â”‚
â”‚                                                              â”‚
â”‚  User Question                                               â”‚
â”‚       â”‚                                                      â”‚
â”‚       â”œâ”€â”€[Naive Path]â”€â”€â–º Embed Query (BGE-small)             â”‚
â”‚       â”‚                       â”‚                              â”‚
â”‚       â””â”€â”€[HyDE Path]â”€â”€â”€â–º LLM Hypothesis â”€â”€â–º Embed Hypothesisâ”‚
â”‚                                   â”‚                          â”‚
â”‚                          FAISS Index Search (top-k)          â”‚
â”‚                                   â”‚                          â”‚
â”‚                          Retrieved Chunks + Scores           â”‚
â”‚                                   â”‚                          â”‚
â”‚                     Format Grounded Prompt                   â”‚
â”‚                                   â”‚                          â”‚
â”‚                     Claude API â†’ Cited Answer                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   INGESTION PIPELINE                         â”‚
â”‚                                                              â”‚
â”‚  PubMed Central PDFs                                         â”‚
â”‚       â”‚                                                      â”‚
â”‚       â–º Text Extraction (PyMuPDF / pypdf)                    â”‚
â”‚       â–º Cleaning (headers, footers, artifacts)               â”‚
â”‚       â–º Chunking (4 strategies tested)                       â”‚
â”‚       â–º Embedding (BAAI/bge-small-en-v1.5, 384-dim)         â”‚
â”‚       â–º FAISS Index (IndexFlatIP, normalized cosine)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Evaluation Results

Evaluated on a custom gold-standard dataset of XX question-answer pairs across clinical assessment literature.

| Configuration | Chunks | Hit@3 | Hit@5 | Hit@10 | MRR |
|---|---|---|---|---|---|
| fixed_512 + naive | XXX | X.XXX | X.XXX | X.XXX | X.XXX |
| fixed_512 + HyDE | XXX | X.XXX | X.XXX | X.XXX | X.XXX |
| semantic + naive | XXX | X.XXX | X.XXX | X.XXX | X.XXX |
| semantic + HyDE | XXX | X.XXX | X.XXX | X.XXX | X.XXX |

**Key findings:**
- HyDE + semantic chunking improved Hit Rate@5 by XX% over the naive baseline
- Semantic chunking alone improved MRR by XX%, confirming that section-boundary splits preserve clinical context better than arbitrary fixed-size windows
- HyDE adds ~Xms latency per query (one additional LLM call) â€” worth the tradeoff for offline analysis but not for real-time clinical use

*Full W&B experiment tracking: [dashboard link]*

## Setup

```bash
# Clone
git clone https://github.com/RRobin711/Clinical-RAG-system.git
cd Clinical-RAG-system

# Environment
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
pip install -e .

# API key
echo 'ANTHROPIC_API_KEY="sk-ant-..."' > .env

# Add PDFs to data/papers/, then run pipeline:
python scripts/01_ingest_pdfs.py
python scripts/02_chunk_texts.py
python scripts/03_build_faiss_index.py
python scripts/04_query.py "What is the reliability of the MADRS?"

# Run evaluation
python scripts/05_evaluate.py --eval_file evaluation/gold_qa.json

# Run experiments
python scripts/06_run_experiments.py --all
python scripts/06_run_experiments.py --compare

# Launch demo
python app.py
```

## Docker

```bash
docker build -t clinical-rag .
docker run -p 7860:7860 -e ANTHROPIC_API_KEY=sk-ant-... clinical-rag
```

## Tests

```bash
pytest tests/ -v
```

## Tech Stack

- **Embeddings:** BAAI/bge-small-en-v1.5 (384-dim, CPU-friendly)
- **Vector Store:** FAISS (IndexFlatIP with L2-normalized embeddings â‰ˆ cosine similarity)
- **LLM:** Claude via Anthropic API
- **Retrieval:** Naive dense retrieval + HyDE (Gao et al., 2022)
- **Evaluation:** Custom Hit@k, MRR metrics with W&B tracking
- **UI:** Gradio
- **CI:** GitHub Actions (pytest on push)
- **Deployment:** Hugging Face Spaces

## Project Structure

```
clinical-rag-system/
â”œâ”€â”€ src/clinical_rag/
â”‚   â”œâ”€â”€ chunking.py              # Day 2: Fixed-size chunking
â”‚   â”œâ”€â”€ chunking_strategies.py   # Day 6: Semantic chunking + HyDE
â”‚   â”œâ”€â”€ evaluation.py            # Day 5: Hit@k, MRR, W&B logging
â”‚   â”œâ”€â”€ indexing.py              # Day 3: FAISS index build/load
â”‚   â”œâ”€â”€ llm.py                   # Day 4: LLM integration
â”‚   â”œâ”€â”€ pipeline.py              # Day 4: End-to-end pipeline
â”‚   â”œâ”€â”€ prompting.py             # Day 4: Grounded prompt formatting
â”‚   â”œâ”€â”€ retriever.py             # Day 3: Dense retrieval
â”‚   â””â”€â”€ settings.py              # Configuration
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ 01_ingest_pdfs.py        # PDF â†’ text extraction
â”‚   â”œâ”€â”€ 02_chunk_texts.py        # Text â†’ chunks
â”‚   â”œâ”€â”€ 03_build_faiss_index.py  # Chunks â†’ FAISS index
â”‚   â”œâ”€â”€ 04_query.py              # End-to-end query
â”‚   â”œâ”€â”€ 05_build_eval_dataset.py # Build gold QA dataset
â”‚   â”œâ”€â”€ 05_evaluate.py           # Run evaluation
â”‚   â””â”€â”€ 06_run_experiments.py    # Experiment matrix runner
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ gold_qa.json             # Gold-standard QA pairs
â”‚   â””â”€â”€ experiment_results/      # Saved experiment outputs
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_rag_system.py       # Pytest suite
â”œâ”€â”€ app.py                       # Gradio demo
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ .github/workflows/test.yml   # CI
â””â”€â”€ README.md
```

## What I Learned

*(Fill this in â€” it becomes your blog post material)*

1. **Chunking matters more than you think.** Fixed 512-char chunks lose context at section boundaries. Semantic chunking that respects paragraph and section structure improves retrieval because clinical papers have highly structured information.

2. **Questions and answers live in different embedding spaces.** "What is the inter-rater reliability of MADRS?" is linguistically far from "The ICC was 0.89 (95% CI: 0.82â€“0.94)." HyDE bridges this gap by generating a hypothetical answer first.

3. **Evaluation catches silent failures.** Without a gold QA dataset, I had no way to know that my retriever was returning plausible-looking but wrong chunks for 30% of queries. The LLM would confidently answer from bad context.

## License

MIT
